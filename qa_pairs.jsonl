{"question": "What are some capabilities of Large Language Models (LLMs)?", "answer": "LLMs can write code, craft emails, and even whip up poetry."}
{"question": "What limitations do LLMs have?", "answer": "LLMs are limited by their training data, which can be outdated or insufficient for niche topics, and they can also produce incorrect information, known as 'hallucinations.'"}
{"question": "What does Retrieval-Augmented Generation (RAG) do?", "answer": "RAG connects the LLM to external knowledge sources at the moment you ask a question, allowing it to use curated notes relevant to the questions."}
{"question": "How does RAG improve LLMs?", "answer": "RAG improves LLMs by providing real-time access to external knowledge sources, enhancing their ability to answer questions accurately."}
{"question": "What is the first step in the process described in the text?", "answer": "The first step is to prep your notes by chopping them into smaller bits and turning them into special number codes (embeddings)."}
{"question": "How does the system find the relevant notes for a question?", "answer": "The system turns the question into number codes and searches the library for note chunks whose codes are most similar to the question's codes."}
{"question": "What is one advantage of RAG in terms of information retrieval?", "answer": "RAG can pull in the latest data, bypassing the LLM\u2019s knowledge cut-off."}
{"question": "How does RAG help reduce hallucinations in LLMs?", "answer": "By giving the LLM facts to work with, it\u2019s less likely to make stuff up."}
{"question": "What is a potential drawback of the retrieval step in RAG?", "answer": "The retrieval step can be slow, adding latency to the response."}
{"question": "Is RAG considered dead according to the text?", "answer": "No, RAG is definitely not dead and is still the go-to for dynamic, massive, or constantly changing knowledge bases."}
{"question": "What does CAG involve in relation to LLMs?", "answer": "CAG involves giving the LLM the entire relevant textbook beforehand to pre-load knowledge into its short-term memory."}
{"question": "What is the purpose of pre-loading knowledge into the LLM?", "answer": "The purpose is to leverage the increasingly large context windows of modern LLMs."}
{"question": "What is the first step in the process of using an LLM for a specific task?", "answer": "The first step is to load the textbook by selecting all relevant documents that fit within the LLM\u2019s context window limit."}
{"question": "What happens during the 'Let it Study' phase?", "answer": "During the 'Let it Study' phase, the LLM processes the preloaded information once and saves its internal understanding in the Key-Value cache."}
{"question": "What is one advantage of CAG regarding answer speed?", "answer": "CAG provides faster answers because there is no real-time retrieval, resulting in lower latency."}
{"question": "How does CAG potentially improve answer consistency?", "answer": "CAG might generate more coherent and contextually accurate answers since the LLM sees all the relevant information upfront."}
{"question": "What is the biggest issue with CAG?", "answer": "The biggest issue with CAG is that it only works if the entire relevant knowledge base can fit into the LLM\u2019s context window."}
{"question": "What happens if the knowledge changes in CAG?", "answer": "If the knowledge changes, you have to redo the whole preloading and caching process, which can be computationally expensive."}
{"question": "What is CAG suitable for?", "answer": "CAG is suitable for querying a single, stable manual or a fixed set of FAQs where speed is paramount and the data volume is manageable."}
{"question": "What does fine-tuning involve?", "answer": "Fine-tuning involves modifying the model\u2019s internal weights based on new examples to give it specialized training for a particular skill, domain, or style."}
{"question": "What is the first step in the process of fine-tuning a language model?", "answer": "The first step is to create a custom curriculum by gathering a dataset of high-quality examples specific to your goal."}
{"question": "What does fine-tuning a pre-trained LLM involve?", "answer": "Fine-tuning involves continuing the training of a pre-trained LLM using a custom dataset to adjust the model\u2019s parameters for better performance on a specific task."}
{"question": "What is one benefit of fine-tuning a language model?", "answer": "Fine-tuning can achieve top-notch results on the specific tasks it was trained for."}
{"question": "How does fine-tuning affect the language model's performance in a specific domain?", "answer": "Fine-tuning makes the LLM fluent in specific jargon, styles, or knowledge areas."}
{"question": "What is a potential downside of specialization in LLMs?", "answer": "A potential downside is the risk of overfitting, where the model becomes too good at the training data and fails on slightly different, real-world examples."}
{"question": "What does 'catastrophic forgetting' refer to in the context of LLM specialization?", "answer": "'Catastrophic forgetting' refers to the phenomenon where specializing makes the LLM worse at general tasks it used to know."}
{"question": "What is RAG best suited for?", "answer": "RAG is best suited for fresh, dynamic information, a large knowledge base, and verifiability."}
{"question": "When should one consider using Fine-Tuning?", "answer": "One should consider using Fine-Tuning when deep expertise, specific style, and peak performance on a narrow task are needed."}
{"question": "What should you consider regarding the size of your knowledge base when choosing a model?", "answer": "A huge knowledge base is suitable for RAG, while a small/medium knowledge base might work with CAG."}
{"question": "What is a recommended approach for teams when integrating different models?", "answer": "Smart teams often combine approaches, such as fine-tuning an LLM for a general domain and using RAG for specific, up-to-date data."}
{"question": "What is the RAG approach for generating an exam from a textbook?", "answer": "The RAG approach involves chopping the book into sections, embedding them, and querying a vector database for relevant chunks to generate questions."}
{"question": "What are the pros and cons of the RAG approach?", "answer": "The pros of the RAG approach are its flexibility and ability to target specific sections easily, while the cons include the potential to miss connections across chapters and added latency from retrieval."}
{"question": "What is a potential advantage of prompting a model to generate questions from a preloaded book?", "answer": "A potential advantage is faster generation and the ability to capture broader context if the whole book fits."}
{"question": "What is a significant drawback of fine-tuning a model on a dataset of question-answer pairs?", "answer": "A significant drawback is the massive data creation effort required, which could involve hundreds or thousands of examples."}
{"question": "What are the main characteristics of RAG, CAG, and Fine-Tuning?", "answer": "RAG is flexible, CAG is fast but limited by size, and Fine-Tuning offers specialization at a high upfront cost."}
{"question": "How can understanding RAG, CAG, and Fine-Tuning help in LLM applications?", "answer": "Understanding their strengths, weaknesses, and unique use cases helps you pick the right approach or combination to enhance your LLM application."}
{"question": "What is Retrieval-Augmented Generation (RAG)?", "answer": "Retrieval-Augmented Generation (RAG) is a method discussed in a Google Cloud article."}
{"question": "What is Cache-Augmented Generation (CAG)?", "answer": "Cache-Augmented Generation (CAG) is explored in a deep dive article on ADaSci."}
